{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def smooth(data, k):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        num_episodes = data.shape[1]\n",
    "        num_runs = data.shape[0]\n",
    "    \n",
    "        smoothed_data = np.zeros((num_runs, num_episodes))\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            if i < k:\n",
    "                smoothed_data[:, i] = np.mean(data[:, :i+1], axis = 1)   \n",
    "            else:\n",
    "                smoothed_data[:, i] = np.mean(data[:, i-k:i+1], axis = 1)    \n",
    "\n",
    "        return smoothed_data\n",
    "    else:\n",
    "        num_episodes = len(data)\n",
    "        num_runs = 1\n",
    "\n",
    "        smoothed_data = np.zeros((num_runs, num_episodes))\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            if i < k:\n",
    "                smoothed_data[:, i] = np.mean(data[:i+1])\n",
    "            else:\n",
    "                smoothed_data[:, i] = np.mean(data[i-k:i+1])\n",
    "        \n",
    "        return smoothed_data\n",
    "\n",
    "\n",
    "# Function to plot result\n",
    "def plot_result(data_name_array, direct=False, k=5):\n",
    "    plt_agent_sweeps = []\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    max_list = []\n",
    "\n",
    "    for data_name in data_name_array:\n",
    "        # load data\n",
    "        if not direct:\n",
    "            filename = 'sum_reward_{}'.format(data_name).replace('.','')\n",
    "            sum_reward_data = np.load('{}/{}.npy'.format(\"results/\", filename))\n",
    "\n",
    "        # smooth data\n",
    "        else:\n",
    "            sum_reward_data = data_name_array[data_name]\n",
    "\n",
    "        smoothed_sum_reward = smooth(data=sum_reward_data, k=k)\n",
    "        max_list.append(max(smoothed_sum_reward[0]))\n",
    "        mean_smoothed_sum_reward = np.mean(smoothed_sum_reward, axis = 0)\n",
    "\n",
    "        plot_x_range = np.arange(0, mean_smoothed_sum_reward.shape[0])\n",
    "        graph_current_agent_sum_reward, = ax.plot(plot_x_range, mean_smoothed_sum_reward[:], label=data_name)\n",
    "        plt_agent_sweeps.append(graph_current_agent_sum_reward)\n",
    "\n",
    "    max_to_hundred = int(math.ceil(max(max_list) / 100.0)) * 100\n",
    "    \n",
    "    ax.legend(handles=plt_agent_sweeps, fontsize = 13)\n",
    "    ax.set_title(\"Learning Curve\", fontsize = 15)\n",
    "    ax.set_xlabel('Episodes', fontsize = 14)\n",
    "    ax.set_ylabel(\"Sum of\\nreward\\nduring\\nepisode\", rotation=0, labelpad=40, fontsize = 14)\n",
    "    ax.set_ylim([-200, max_to_hundred])\n",
    "    plt.show()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "HEIGHT = 20      # number of steps vertically from wall to wall of screen\n",
    "WIDTH = 20       # number of steps horizontally from wall to wall of screen\n",
    "PIXEL_H = 20*HEIGHT  # pixel height + border on both sides\n",
    "PIXEL_W = 20*WIDTH   # pixel width + border on both sides\n",
    "\n",
    "SLEEP = 0.2     # time to wait between steps\n",
    "\n",
    "GAME_TITLE = 'Snake'\n",
    "BG_COLOR = 'white'\n",
    "\n",
    "SNAKE_SHAPE = 'square'\n",
    "SNAKE_COLOR = 'black'\n",
    "SNAKE_START_LOC_H = 0\n",
    "SNAKE_START_LOC_V = 0\n",
    "\n",
    "APPLE_SHAPE = 'circle'\n",
    "APPLE_COLOR = 'green'\n",
    "\n",
    "class Snake(gym.Env):\n",
    "\n",
    "    def __init__(self, human=False, env_info={'state_space':None}):\n",
    "        super(Snake, self).__init__()\n",
    "\n",
    "        self.done = False\n",
    "        self.seed()\n",
    "        self.reward = 0\n",
    "        self.action_space = 4\n",
    "        self.state_space = 12\n",
    "\n",
    "        self.total, self.maximum = 0, 0\n",
    "        self.human = human\n",
    "        self.env_info = env_info\n",
    "\n",
    "        ## GAME CREATION WITH TURTLE (RENDER?)\n",
    "        # screen/background\n",
    "        self.win = turtle.Screen()\n",
    "        self.win.title(GAME_TITLE)\n",
    "        self.win.bgcolor(BG_COLOR)\n",
    "        self.win.tracer(0)\n",
    "        self.win.setup(width=PIXEL_W+32, height=PIXEL_H+32)\n",
    "                \n",
    "        # snake\n",
    "        self.snake = turtle.Turtle()\n",
    "        self.snake.shape(SNAKE_SHAPE)\n",
    "        self.snake.speed(0)\n",
    "        self.snake.penup()\n",
    "        self.snake.color(SNAKE_COLOR)\n",
    "        self.snake.goto(SNAKE_START_LOC_H, SNAKE_START_LOC_V)\n",
    "        self.snake.direction = 'stop'\n",
    "        # snake body, add first element (for location of snake's head)\n",
    "        self.snake_body = []\n",
    "        self.add_to_body()\n",
    "\n",
    "        # apple\n",
    "        self.apple = turtle.Turtle()\n",
    "        self.apple.speed(0)\n",
    "        self.apple.shape(APPLE_SHAPE)\n",
    "        self.apple.color(APPLE_COLOR)\n",
    "        self.apple.penup()\n",
    "        self.move_apple(first=True)\n",
    "\n",
    "        # distance between apple and snake\n",
    "        self.dist = math.sqrt((self.snake.xcor()-self.apple.xcor())**2 + (self.snake.ycor()-self.apple.ycor())**2)\n",
    "\n",
    "        # score\n",
    "        self.score = turtle.Turtle()\n",
    "        self.score.speed(0)\n",
    "        self.score.color('black')\n",
    "        self.score.penup()\n",
    "        self.score.hideturtle()\n",
    "        self.score.goto(0, 100)\n",
    "        self.score.write(f\"Total: {self.total}   Highest: {self.maximum}\", align='center', font=('Courier', 18, 'normal'))\n",
    "\n",
    "        # control\n",
    "        self.win.listen()\n",
    "        self.win.onkey(self.go_up, 'Up')\n",
    "        self.win.onkey(self.go_right, 'Right')\n",
    "        self.win.onkey(self.go_down, 'Down')\n",
    "        self.win.onkey(self.go_left, 'Left')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def random_coordinates(self):\n",
    "        apple_x = random.randint(-WIDTH/2, WIDTH/2)\n",
    "        apple_y = random.randint(-HEIGHT/2, HEIGHT/2)\n",
    "        return apple_x, apple_y\n",
    "    \n",
    "    def move_snake(self):\n",
    "        if self.snake.direction == 'stop':\n",
    "            self.reward = 0\n",
    "        if self.snake.direction == 'up':\n",
    "            y = self.snake.ycor()\n",
    "            self.snake.sety(y + 20)\n",
    "        if self.snake.direction == 'right':\n",
    "            x = self.snake.xcor()\n",
    "            self.snake.setx(x + 20)\n",
    "        if self.snake.direction == 'down':\n",
    "            y = self.snake.ycor()\n",
    "            self.snake.sety(y - 20)\n",
    "        if self.snake.direction == 'left':\n",
    "            x = self.snake.xcor()\n",
    "            self.snake.setx(x - 20)\n",
    "        \n",
    "    \n",
    "    def go_up(self):\n",
    "        if self.snake.direction != \"down\":\n",
    "            self.snake.direction = \"up\"\n",
    "    \n",
    "    \n",
    "    def go_down(self):\n",
    "        if self.snake.direction != \"up\":\n",
    "            self.snake.direction = \"down\"\n",
    "    \n",
    "    \n",
    "    def go_right(self):\n",
    "        if self.snake.direction != \"left\":\n",
    "            self.snake.direction = \"right\"\n",
    "    \n",
    "    \n",
    "    def go_left(self):\n",
    "        if self.snake.direction != \"right\":\n",
    "            self.snake.direction = \"left\"\n",
    "\n",
    "\n",
    "    def move_apple(self, first=False):\n",
    "        if first or self.snake.distance(self.apple) < 20:    \n",
    "            while True:\n",
    "                self.apple.x, self.apple.y = self.random_coordinates()\n",
    "                self.apple.goto(round(self.apple.x*20), round(self.apple.y*20))\n",
    "                if not self.body_check_apple():\n",
    "                    break\n",
    "            if not first:\n",
    "                self.update_score()\n",
    "                self.add_to_body()\n",
    "            first = False\n",
    "            return True\n",
    "\n",
    "\n",
    "    def update_score(self):\n",
    "        self.total += 1\n",
    "        if self.total >= self.maximum:\n",
    "            self.maximum = self.total\n",
    "        self.score.clear()\n",
    "        self.score.write(f\"Total: {self.total}   Highest: {self.maximum}\", align='center', font=('Courier', 18, 'normal'))\n",
    "\n",
    "\n",
    "    def reset_score(self):\n",
    "        self.score.clear()\n",
    "        self.total = 0\n",
    "        self.score.write(f\"Total: {self.total}   Highest: {self.maximum}\", align='center', font=('Courier', 18, 'normal'))\n",
    "                    \n",
    "\n",
    "    def add_to_body(self):\n",
    "        body = turtle.Turtle()\n",
    "        body.speed(0)\n",
    "        body.shape('square')\n",
    "        body.color('black')\n",
    "        body.penup()\n",
    "        self.snake_body.append(body)\n",
    "        \n",
    "\n",
    "    def move_snakebody(self):\n",
    "        if len(self.snake_body) > 0:\n",
    "            for index in range(len(self.snake_body)-1, 0, -1):\n",
    "                x = self.snake_body[index-1].xcor()\n",
    "                y = self.snake_body[index-1].ycor()\n",
    "                self.snake_body[index].goto(x, y)\n",
    "\n",
    "            self.snake_body[0].goto(self.snake.xcor(), self.snake.ycor())\n",
    "        \n",
    "    \n",
    "    def measure_distance(self):\n",
    "        self.prev_dist = self.dist\n",
    "        self.dist = math.sqrt((self.snake.xcor()-self.apple.xcor())**2 + (self.snake.ycor()-self.apple.ycor())**2)\n",
    "\n",
    "\n",
    "    def body_check_snake(self):\n",
    "        if len(self.snake_body) > 1:\n",
    "            for body in self.snake_body[1:]:\n",
    "                if body.distance(self.snake) < 20:\n",
    "                    self.reset_score()\n",
    "                    return True     \n",
    "\n",
    "    def body_check_apple(self):\n",
    "        if len(self.snake_body) > 0:\n",
    "            for body in self.snake_body[:]:\n",
    "                if body.distance(self.apple) < 20:\n",
    "                    return True\n",
    "\n",
    "    def wall_check(self):\n",
    "        if self.snake.xcor() > 200 or self.snake.xcor() < -200 or self.snake.ycor() > 200 or self.snake.ycor() < -200:\n",
    "            self.reset_score()\n",
    "            return True\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.human:\n",
    "            time.sleep(1)\n",
    "        for body in self.snake_body:\n",
    "            body.goto(1000, 1000)\n",
    "\n",
    "        self.snake_body = []\n",
    "        self.snake.goto(SNAKE_START_LOC_H, SNAKE_START_LOC_V)\n",
    "        self.snake.direction = 'stop'\n",
    "        self.reward = 0\n",
    "        self.total = 0\n",
    "        self.done = False\n",
    "\n",
    "        state = self.get_state()\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    def run_game(self):\n",
    "        reward_given = False\n",
    "        self.win.update()\n",
    "        self.move_snake()\n",
    "        if self.move_apple():\n",
    "            self.reward = 10\n",
    "            reward_given = True\n",
    "        self.move_snakebody()\n",
    "        self.measure_distance()\n",
    "        if self.body_check_snake():\n",
    "            self.reward = -100\n",
    "            reward_given = True\n",
    "            self.done = True\n",
    "            if self.human:\n",
    "                self.reset()\n",
    "        if self.wall_check():\n",
    "            self.reward = -100\n",
    "            reward_given = True\n",
    "            self.done = True\n",
    "            if self.human:\n",
    "                self.reset()\n",
    "        if not reward_given:\n",
    "            if self.dist < self.prev_dist:\n",
    "                self.reward = 1\n",
    "            else:\n",
    "                self.reward = -1\n",
    "        # time.sleep(0.1)\n",
    "        if self.human:\n",
    "            time.sleep(SLEEP)\n",
    "            state = self.get_state()\n",
    "\n",
    "    \n",
    "    # AI agent\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.go_up()\n",
    "        if action == 1:\n",
    "            self.go_right()\n",
    "        if action == 2:\n",
    "            self.go_down()\n",
    "        if action == 3:\n",
    "            self.go_left()\n",
    "        self.run_game()\n",
    "        state = self.get_state()\n",
    "        return state, self.reward, self.done, {}\n",
    "\n",
    "\n",
    "    def get_state(self):\n",
    "        # snake coordinates abs\n",
    "        self.snake.x, self.snake.y = self.snake.xcor()/WIDTH, self.snake.ycor()/HEIGHT   \n",
    "        # snake coordinates scaled 0-1\n",
    "        self.snake.xsc, self.snake.ysc = self.snake.x/WIDTH+0.5, self.snake.y/HEIGHT+0.5\n",
    "        # apple coordintes scaled 0-1 \n",
    "        self.apple.xsc, self.apple.ysc = self.apple.x/WIDTH+0.5, self.apple.y/HEIGHT+0.5\n",
    "\n",
    "        # wall check\n",
    "        if self.snake.y >= HEIGHT/2:\n",
    "            wall_up, wall_down = 1, 0\n",
    "        elif self.snake.y <= -HEIGHT/2:\n",
    "            wall_up, wall_down = 0, 1\n",
    "        else:\n",
    "            wall_up, wall_down = 0, 0\n",
    "        if self.snake.x >= WIDTH/2:\n",
    "            wall_right, wall_left = 1, 0\n",
    "        elif self.snake.x <= -WIDTH/2:\n",
    "            wall_right, wall_left = 0, 1\n",
    "        else:\n",
    "            wall_right, wall_left = 0, 0\n",
    "\n",
    "        # body close\n",
    "        body_up = []\n",
    "        body_right = []\n",
    "        body_down = []\n",
    "        body_left = []\n",
    "        if len(self.snake_body) > 3:\n",
    "            for body in self.snake_body[3:]:\n",
    "                if body.distance(self.snake) == 20:\n",
    "                    if body.ycor() < self.snake.ycor():\n",
    "                        body_down.append(1)\n",
    "                    elif body.ycor() > self.snake.ycor():\n",
    "                        body_up.append(1)\n",
    "                    if body.xcor() < self.snake.xcor():\n",
    "                        body_left.append(1)\n",
    "                    elif body.xcor() > self.snake.xcor():\n",
    "                        body_right.append(1)\n",
    "        \n",
    "        if len(body_up) > 0: body_up = 1\n",
    "        else: body_up = 0\n",
    "        if len(body_right) > 0: body_right = 1\n",
    "        else: body_right = 0\n",
    "        if len(body_down) > 0: body_down = 1\n",
    "        else: body_down = 0\n",
    "        if len(body_left) > 0: body_left = 1\n",
    "        else: body_left = 0\n",
    "\n",
    "        # state: apple_up, apple_right, apple_down, apple_left, obstacle_up, obstacle_right, obstacle_down, obstacle_left, direction_up, direction_right, direction_down, direction_left\n",
    "        if self.env_info['state_space'] == 'coordinates':\n",
    "            state = [self.apple.xsc, self.apple.ysc, self.snake.xsc, self.snake.ysc, \\\n",
    "                    int(wall_up or body_up), int(wall_right or body_right), int(wall_down or body_down), int(wall_left or body_left), \\\n",
    "                    int(self.snake.direction == 'up'), int(self.snake.direction == 'right'), int(self.snake.direction == 'down'), int(self.snake.direction == 'left')]\n",
    "        elif self.env_info['state_space'] == 'no direction':\n",
    "            state = [int(self.snake.y < self.apple.y), int(self.snake.x < self.apple.x), int(self.snake.y > self.apple.y), int(self.snake.x > self.apple.x), \\\n",
    "                    int(wall_up or body_up), int(wall_right or body_right), int(wall_down or body_down), int(wall_left or body_left), \\\n",
    "                    0, 0, 0, 0]\n",
    "        elif self.env_info['state_space'] == 'no body knowledge':\n",
    "            state = [int(self.snake.y < self.apple.y), int(self.snake.x < self.apple.x), int(self.snake.y > self.apple.y), int(self.snake.x > self.apple.x), \\\n",
    "                    wall_up, wall_right, wall_down, wall_left, \\\n",
    "                    int(self.snake.direction == 'up'), int(self.snake.direction == 'right'), int(self.snake.direction == 'down'), int(self.snake.direction == 'left')]\n",
    "        else:\n",
    "            state = [int(self.snake.y < self.apple.y), int(self.snake.x < self.apple.x), int(self.snake.y > self.apple.y), int(self.snake.x > self.apple.x), \\\n",
    "                    int(wall_up or body_up), int(wall_right or body_right), int(wall_down or body_down), int(wall_left or body_left), \\\n",
    "                    int(self.snake.direction == 'up'), int(self.snake.direction == 'right'), int(self.snake.direction == 'down'), int(self.snake.direction == 'left')]\n",
    "            \n",
    "        # print(state)\n",
    "        return state\n",
    "\n",
    "    def bye(self):\n",
    "        self.win.bye()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final state before dying: [[1 0 0 1 0 1 0 0 0 1 0 0]]\n",
      "episode: 1/50, score: -104\n",
      "final state before dying: [[1 0 0 1 0 1 0 0 0 1 0 0]]\n",
      "episode: 2/50, score: -96\n",
      "final state before dying: [[1 1 0 0 0 0 0 1 0 0 0 1]]\n",
      "episode: 3/50, score: -103\n",
      "final state before dying: [[1 1 0 0 0 0 1 0 0 0 1 0]]\n",
      "episode: 4/50, score: -111\n",
      "final state before dying: [[1 1 0 0 0 0 0 1 0 0 0 1]]\n",
      "episode: 5/50, score: -111\n",
      "final state before dying: [[1 0 0 1 0 1 0 0 0 1 0 0]]\n",
      "episode: 6/50, score: -108\n",
      "final state before dying: [[1 0 0 1 0 1 0 0 0 1 0 0]]\n",
      "episode: 7/50, score: -112\n",
      "final state before dying: [[1 1 0 0 0 0 0 1 0 0 0 1]]\n",
      "episode: 8/50, score: -102\n",
      "final state before dying: [[1 0 0 1 0 1 0 0 1 0 0 0]]\n",
      "episode: 9/50, score: -101\n",
      "final state before dying: [[0 0 0 1 1 0 0 0 0 0 0 1]]\n",
      "episode: 10/50, score: -89\n",
      "final state before dying: [[1 0 0 1 0 1 0 0 0 1 0 0]]\n",
      "episode: 11/50, score: -110\n",
      "final state before dying: [[0 1 0 0 1 0 0 0 1 0 0 0]]\n",
      "episode: 12/50, score: -91\n",
      "final state before dying: [[0 1 0 0 1 0 0 0 0 1 0 0]]\n",
      "episode: 13/50, score: -90\n",
      "final state before dying: [[0 0 1 1 1 1 0 0 0 1 0 0]]\n",
      "episode: 14/50, score: -85\n",
      "final state before dying: [[0 1 1 0 0 0 0 1 0 0 0 1]]\n",
      "episode: 15/50, score: -103\n",
      "final state before dying: [[0 1 1 0 0 0 0 1 0 0 0 1]]\n",
      "episode: 16/50, score: -95\n",
      "final state before dying: [[0 1 1 0 1 0 0 0 0 1 0 0]]\n",
      "episode: 17/50, score: 136\n",
      "final state before dying: [[0 1 1 0 0 1 0 0 0 1 0 0]]\n",
      "episode: 18/50, score: 163\n",
      "final state before dying: [[0 0 1 0 0 0 1 0 0 0 1 0]]\n",
      "episode: 19/50, score: 643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12524/2271683429.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;31m#     env = Snake(env_info=env_info)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSnake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[0msum_of_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum_of_rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12524/2271683429.py\u001b[0m in \u001b[0;36mtrain_dqn\u001b[1;34m(episode, env)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'final state before dying: {str(prev_state)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12524/2271683429.py\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mtargets_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Deep Q Network \"\"\"\n",
    "\n",
    "    def __init__(self, env, params):\n",
    "\n",
    "        self.action_space = env.action_space\n",
    "        self.state_space = env.state_space\n",
    "        self.epsilon = params['epsilon'] \n",
    "        self.gamma = params['gamma'] \n",
    "        self.batch_size = params['batch_size'] \n",
    "        self.epsilon_min = params['epsilon_min'] \n",
    "        self.epsilon_decay = params['epsilon_decay'] \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.layer_sizes = params['layer_sizes']\n",
    "        self.memory = deque(maxlen=2500)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        for i in range(len(self.layer_sizes)):\n",
    "            if i == 0:\n",
    "                model.add(Dense(self.layer_sizes[i], input_shape=(self.state_space,), activation='relu'))\n",
    "            else:\n",
    "                model.add(Dense(self.layer_sizes[i], activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='softmax'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "def train_dqn(episode, env):\n",
    "\n",
    "    sum_of_rewards = []\n",
    "    agent = DQN(env, params)\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, env.state_space))\n",
    "        score = 0\n",
    "        max_steps = 10000\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            # print(action)\n",
    "            prev_state = state\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, env.state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if params['batch_size'] > 1:\n",
    "                agent.replay()\n",
    "            if done:\n",
    "                print(f'final state before dying: {str(prev_state)}')\n",
    "                print(f'episode: {e+1}/{episode}, score: {score}')\n",
    "                break\n",
    "        sum_of_rewards.append(score)\n",
    "    return sum_of_rewards\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    params = dict()\n",
    "    params['name'] = None\n",
    "    params['epsilon'] = 1\n",
    "    params['gamma'] = .95\n",
    "    params['batch_size'] = 500\n",
    "    params['epsilon_min'] = .01\n",
    "    params['epsilon_decay'] = .995\n",
    "    params['learning_rate'] = 0.00025\n",
    "    params['layer_sizes'] = [128, 128, 128]\n",
    "\n",
    "    results = dict()\n",
    "    ep = 50\n",
    "\n",
    "    # for batchsz in [1, 10, 100, 1000]:\n",
    "    #     print(batchsz)\n",
    "    #     params['batch_size'] = batchsz\n",
    "    #     nm = ''\n",
    "    #     params['name'] = f'Batchsize {batchsz}'\n",
    "    env_infos = {'States: only walls':{'state_space':'no body knowledge'}, 'States: direction 0 or 1':{'state_space':''}, 'States: coordinates':{'state_space':'coordinates'}, 'States: no direction':{'state_space':'no direction'}}\n",
    "\n",
    "    # for key in env_infos.keys():\n",
    "    #     params['name'] = key\n",
    "    #     env_info = env_infos[key]\n",
    "    #     print(env_info)\n",
    "    #     env = Snake(env_info=env_info)\n",
    "    env = Snake()\n",
    "    sum_of_rewards = train_dqn(ep, env)\n",
    "    results[params['name']] = sum_of_rewards\n",
    "    \n",
    "    plot_result(results, direct=True, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
